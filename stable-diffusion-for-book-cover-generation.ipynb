{"cells":[{"cell_type":"markdown","metadata":{"id":"tAZq3vFDcFiT"},"source":["# Textual-inversion fine-tuning for Stable Diffusion using d🧨ffusers \n","\n","This notebook shows how to \"teach\" Stable Diffusion a new concept via textual-inversion using 🤗 Hugging Face [🧨 Diffusers library](https://github.com/huggingface/diffusers). \n","\n","![Textual Inversion example](https://textual-inversion.github.io/static/images/editing/colorful_teapot.JPG)\n","_By using just 3-5 images you can teach new concepts to Stable Diffusion and personalize the model on your own images_ \n","\n","For a general introduction to the Stable Diffusion model please refer to this [colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KbzZ9xe6dWwf"},"source":["## Initial setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"30lu8LWXmg5j","outputId":"19b7efac-8315-44ff-89d4-5f292b34d41a","trusted":true},"outputs":[],"source":["#@title Install the required libs\n","%pip install -qq diffusers[\"training\"]==0.7.2\n","%pip install -qq transformers==4.24.0 ftfy\n","%pip install -qq \"ipywidgets>=7,<8\"\n","%pip install wandb\n","%pip install kornia\n","%pip install bitsandbytes\n","#0.10 version doesn't contain login()\n","!pip install huggingface_hub==0.11.1\n","#deepspeed\n","# !pip install torch==1.12.1 --extra-index-url https://download.pytorch.org/whl/cu116 --upgrade\n","# !pip install deepspeed==0.7.4 --upgrade\n","# !pip install diffusers==0.6.0 triton==2.0.0.dev20221005 --upgrade\n","# !pip install transformers[sentencepiece]==4.24.0 accelerate --upgrade\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"24zwrNSBm4A3","outputId":"ca86aff9-110b-4cd9-d68d-a35fd56c36bd","trusted":true},"outputs":[],"source":["#@title Login to the Hugging Face Hub\n","#@markdown Add a token with the \"Write Access\" role to be able to add your trained concept to the [Library of Concepts](https://huggingface.co/sd-concepts-library)\n","from huggingface_hub import login\n","login(\"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\")\n","# notebook_login()\n","# from google.colab import drive\n","# drive.mount(\"/content/drive\",force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1_h0kO-VnQog","outputId":"530fc822-cdad-4642-9e8b-37f9651dbd9d","trusted":true},"outputs":[],"source":["#@title Import required libraries\n","# %pip install protobuf==3.20.* #For deepspe\n","\n","import argparse\n","import itertools\n","import math\n","import os\n","import random\n","import numpy as np\n","import torch,torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.checkpoint\n","from torch.utils.data import Dataset\n","import torchvision\n","import PIL\n","from accelerate import Accelerator\n","from accelerate.logging import get_logger\n","from accelerate.utils import set_seed\n","from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n","from diffusers.hub_utils import init_git_repo, push_to_hub\n","from diffusers.optimization import get_scheduler\n","from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n","from PIL import Image\n","from tqdm.auto import tqdm\n","from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer,TrainingArguments\n","import kornia.augmentation as K#augmentaiton\n","import pandas as pd\n","import wandb\n","def image_grid(imgs, rows, cols):\n","    assert len(imgs) == rows*cols\n","\n","    w, h = imgs[0].size\n","    grid = Image.new('RGB', size=(cols*w, rows*h))\n","    grid_w, grid_h = grid.size\n","    \n","    for i, img in enumerate(imgs):\n","        grid.paste(img, box=(i%cols*w, i//cols*h))\n","    return grid\n","    \n","#For reproducibility\n","def set_seed(seed: int = 42) -> None:\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    # When running on the CuDNN backend, two further options must be set\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    # Set a fixed value for the hash seed\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    print(f\"Random seed set as {seed}\")\n","global_seed=42\n","set_seed(global_seed)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"If5Jswe526QP","trusted":true},"outputs":[],"source":["#@markdown `pretrained_model_name_or_path` which Stable Diffusion checkpoint you want to use\n","#pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\" #@param {type:\"string\"}\n","pretrained_model_name_or_path =\"CompVis/stable-diffusion-v1-4\"\n","data_root=\"/kaggle/input/goodreads-best-books\"\n","label_root=\"/kaggle/input/goodreads-best-book-cleaned-version\"\n"]},{"cell_type":"markdown","metadata":{"id":"EuFP688UEwQR"},"source":["### Create Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u4c1vbVfnmLf","trusted":true},"outputs":[],"source":["#@title Setup the prompt templates for training \n","book_cover_templates=[#the first entry is for \"highly legible text\"\n","    \"A {} book cover with author {}, book title {} \",\n","    #repeat some prompts to give model prior knowledge about book cover styles\n","    \"A {} book cover written by author {} with book title {} \",\n","#     \"A {} simple book cover with author {}, book title {} \",\n","#     \"A plain {} book cover with author {}. The book title is{} \",\n","#     \"A {} vivid book cover with author {}, book title {} \",\n","    \"A  {} book cover with author name:{}, book title: {}\",\n","# #     \"We are going to create a clear, {}, highly detailed book cover with author named {}, and book title is '{}'\",\n","#     \"An intricate {}, book cover including book author:{}, book title: '{}'\",\n","#     \"A detailed, {}, book cover with {} ,written by author {}\",\n","#     \"A creative, colorful {}, book cover written by {}. The book title is {}, \",\n","#     \"A {} old-fashioned, plain book cover written by {}. The book title is {}\",\n","#     \"A simple, {}, old-fashioned book cover with author name {}, book title {} \",\n","#     \"A simple, {}, plain book cover with author name {}, book title {} \",\n","    \"A detailed {} book cover with author {} and book title {} \"\n","    \n","]\n","#TODO: add more to match the number of templates\n","summary_placeholders=[\n","    \", and summary: {}\",\n","    ', and abstract: {}',\n","    \",summary: {}\",\n","    \", the book describes that {}\",\n","    \", book discription: {}\",\n","    \", main story: {}\",\n","    \", the book is mainly about {}\",\n","    \", and main story: {}\",\n","    \"and book abstract: {}\",\n","    \", and book description: {}\"\n","]\n","test_templates=[#the first entry is for \"highly legible text\"\n","    \"A {} book cover with author {}, book title {} \",\n","    #repeat some prompts to give model prior knowledge about book cover styles\n","    \"A {} book cover written by author {} with book title {} \",\n","    \"A {} simple book cover with author {}, book title {} \",\n","    \"A plain {} book cover with author {}. The book title is{} \",\n","    \"A {} vivid book cover with author {}, book title {} \",\n","    \"A  {} book cover with author name:{}, book title: {}\",\n","#     \"We are going to create a clear, {}, highly detailed book cover with author named {}, and book title is '{}'\",\n","    \"An intricate {}, book cover including book author:{}, book title: '{}'\",\n","    \"A detailed, {}, book cover with {} ,written by author {}\",\n","    \"A creative, colorful {}, book cover written by {}. The book title is {}, \",\n","    \"A {} old-fashioned, plain book cover written by {}. The book title is {}\",\n","    \"A simple, {}, old-fashioned book cover with author name {}, book title {} \",\n","    \"A simple, {}, plain book cover with author name {}, book title {} \",\n","    \"A detailed {} book cover with author {} and book title {} \"\n","    \n","]\n","#pad to the same length \n","for i in range(len(summary_placeholders),len(test_templates)):\n","  summary_placeholders+=[random.choice(summary_placeholders)]\n","summary_placeholders=summary_placeholders[:len(test_templates)]\n","\n","# imagenet_templates_small = [\n","#     \"a photo of a {}\",\n","#     \"a rendering of a {}\",\n","#     \"a cropped photo of the {}\",\n","#     \"the photo of a {}\",\n","#     \"a photo of a clean {}\",\n","#     \"a photo of a dirty {}\",\n","#     \"a dark photo of the {}\",\n","#     \"a photo of my {}\",\n","#     \"a photo of the cool {}\",\n","#     \"a close-up photo of a {}\",\n","#     \"a bright photo of the {}\",\n","#     \"a cropped photo of a {}\",\n","#     \"a photo of the {}\",\n","#     \"a good photo of the {}\",\n","#     \"a photo of one {}\",\n","#     \"a close-up photo of the {}\",\n","#     \"a rendition of the {}\",\n","#     \"a photo of the clean {}\",\n","#     \"a rendition of a {}\",\n","#     \"a photo of a nice {}\",\n","#     \"a good photo of a {}\",\n","#     \"a photo of the nice {}\",\n","#     \"a photo of the small {}\",\n","#     \"a photo of the weird {}\",\n","#     \"a photo of the large {}\",\n","#     \"a photo of a cool {}\",\n","#     \"a photo of a small {}\",\n","# ]\n","\n","# imagenet_style_templates_small = [\n","#     \"a painting in the style of {}\",\n","#     \"a rendering in the style of {}\",\n","#     \"a cropped painting in the style of {}\",\n","#     \"the painting in the style of {}\",\n","#     \"a clean painting in the style of {}\",\n","#     \"a dirty painting in the style of {}\",\n","#     \"a dark painting in the style of {}\",\n","#     \"a picture in the style of {}\",\n","#     \"a cool painting in the style of {}\",\n","#     \"a close-up painting in the style of {}\",\n","#     \"a bright painting in the style of {}\",\n","#     \"a cropped painting in the style of {}\",\n","#     \"a good painting in the style of {}\",\n","#     \"a close-up painting in the style of {}\",\n","#     \"a rendition in the style of {}\",\n","#     \"a nice painting in the style of {}\",\n","#     \"a small painting in the style of {}\",\n","#     \"a weird painting in the style of {}\",\n","#     \"a large painting in the style of {}\",\n","# ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fcA-kMQblqUe","trusted":true},"outputs":[],"source":["#@title Training hyperparameters \n","hyperparam = {\n","    \"learning_rate\": 5e-6, #original: 5e-4\n","    \"scale_lr\": False,\n","    \"epochs\": 2,\n","    \"train_batch_size\": 1,\n","    \"gradient_accumulation_steps\": 8,\n","    \"seed\": global_seed,\n","    \"weight_decay\": 1e-3,\n","    # \"noise_scheduler\": \"DDIM\",\n","    \"pretrained_model_name_or_path\": pretrained_model_name_or_path,\n","    \"output_dir\": \"./model\",\n","    \"training_dataset_size\":500,\n","    \"train_unet\": False,\n","    \"train_text_encoder\": True,\n","    \"num_templates\": len(book_cover_templates),\n","    \"include_summary\": False,#True to add book summary to prompts\n","    \"templates\" : book_cover_templates\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xp2InXqXW8aY","outputId":"32997303-8897-4243-9a83-e4b75a03272e","trusted":true},"outputs":[],"source":["#@title Load the Stable Diffusion model\n","tokenizer = CLIPTokenizer.from_pretrained(\n","    pretrained_model_name_or_path,\n","    subfolder=\"tokenizer\",\n","    use_auth_token=True,\n",")\n","\n","# Load models and create wrapper for stable diffusion\n","text_encoder = CLIPTextModel.from_pretrained(\n","    pretrained_model_name_or_path, subfolder=\"text_encoder\"\n","    , use_auth_token=True\n",")\n","vae = AutoencoderKL.from_pretrained(\n","    pretrained_model_name_or_path, subfolder=\"vae\"\n","    , use_auth_token=True\n",")\n","unet = UNet2DConditionModel.from_pretrained(\n","    pretrained_model_name_or_path, subfolder=\"unet\"\n","    , use_auth_token=True\n",")\n","#test model with out of max_length token sequence \n","\n","# input=tokenizer(book_df.loc[7202]['book_desc'], return_tensors=\"pt\").input_ids[:,:999]\n","# print(input.shape)\n","# print(\"Test encode above max_length(77) text\",text_encoder(input))\n","\n","\n","tokenizer.tokenize(\" . \"),tokenizer.decode(tokenizer(\"谁\")['input_ids']),tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"谁\")),tokenizer.convert_tokens_to_ids('.'),tokenizer.encode(tokenizer.tokenize(\".\"))==tokenizer.encode(\".\"),tokenizer.decode(tokenizer.encode(\".\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T06:27:18.356266Z","iopub.status.busy":"2022-12-01T06:27:18.355862Z","iopub.status.idle":"2022-12-01T06:27:19.318793Z","shell.execute_reply":"2022-12-01T06:27:19.317772Z","shell.execute_reply.started":"2022-12-01T06:27:18.356225Z"},"id":"2ntpEpVfnd-0","outputId":"d7437541-b405-4225-8897-86d383419b62","trusted":true},"outputs":[],"source":["#@title Setup the dataset and train loader\n","\n","used_times=[]\n","class TextualInversionDataset(Dataset):\n","    def __init__(\n","        self,\n","        data_root,\n","        tokenizer,\n","        # learnable_property=\"object\",  # [object, style]\n","        size=512,\n","        training_size=1000,#use a subset of the training set to save time\n","        interpolation=\"bicubic\",\n","        flip_prob=0,\n","        test_speed=False,\n","        include_desc=False,\n","        summerize_length:int=\"max length of summerized book description\", #not implemented\n","        legible_text_prob=0,#add \"legible text\" to prompt\n","    ):\n","        \n","        self.data_root = data_root\n","        self.image_path=data_root+\"/images/images\"\n","        #changed path for kaggle \n","        self.df=pd.read_csv(os.path.join(label_root,\"df_train.csv\")).iloc[:training_size]\n","        # self.df.set_index(self.df.columns[0],drop=True,inplace=True)\n","        self.tokenizer = tokenizer\n","        # self.learnable_property = learnable_property\n","        # self.size = size\n","        self.flip_prob = flip_prob\n","        self.test_speed=test_speed\n","        self.include_desc=include_desc\n","        self.summerize_length=summerize_length\n","        self.transform = nn.Sequential( \n","            K.RandomHorizontalFlip(p=self.flip_prob)\n","        )\n","        self.legible_text_prob=legible_text_prob\n","        # self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n","        self.size=size\n","        self._length = len(self.df)\n","        \n","        self.interpolation = {\n","            \"linear\": PIL.Image.LINEAR,\n","            \"bilinear\": PIL.Image.BILINEAR,\n","            \"bicubic\": PIL.Image.BICUBIC,\n","            \"lanczos\": PIL.Image.LANCZOS,\n","        }[interpolation]\n","        if self.include_desc:\n","          self.templates = [str1+str2 for str1, str2 in zip(book_cover_templates,summary_placeholders) ]\n","        else:\n","          self.templates=book_cover_templates\n","    \n","        \n","        print(\"dataset.self.tokenizer.model_max_length:\",self.tokenizer.model_max_length)\n","        print(\"dataset.self.tokenizer.truncation_side\",self.tokenizer.truncation_side)\n","\n","    def __len__(self):\n","        return self._length\n","\n","    def __getitem__(self, i):\n","        if self.test_speed:\n","            import time\n","            start_time=time.time()\n","\n","        example = {}\n","        # print(self.df.head())\n","        # print(self.df.iloc[i][self.df.columns[0]])\n","        # print(self.df.columns[0])\n","        #first column is the index of the data point\n","        image = Image.open(os.path.join(self.image_path,str(self.df[self.df.columns[0]].iloc[i])+\".jpg\"))\n","\n","        if not image.mode == \"RGB\":\n","            image = image.convert(\"RGB\")\n","\n","        #randomly choose a prompt\n","        legible_text,author,title,description=(\"\",self.df.iloc[i]['book_authors'], self.df.iloc[i]['book_title'], self.df.iloc[i]['book_desc'])\n","        if random.random()<=self.legible_text_prob:\n","            legible_text=\"legible text\"\n","            \n","        #debug\n","        try:\n","          template=random.choice(self.templates)\n","          if self.include_desc:\n","            text = template.format(legible_text,author,title,description)\n","          else:\n","            text = template.format(legible_text,author,title)\n","        except Exception as e:\n","            print(e)\n","            print(template)\n","          \n","        example[\"input_ids\"] = self.tokenizer(\n","            text,\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=self.tokenizer.model_max_length,\n","            return_tensors=\"pt\",\n","        ).input_ids[0]\n","\n","\n","\n","        # default to score-sde preprocessing\n","        img = np.array(image).astype(np.uint8)\n","        image = Image.fromarray(img)\n","        image = image.resize((self.size, self.size), resample=self.interpolation)\n","        image = np.array(image).astype(np.uint8)\n","        \n","        image = (image / 127.5 - 1.0).astype(np.float32)\n","    \n","        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n","        #Apply data augmentation \n","        #Kornia K.RandomHorizontalFlip() unsqueezes the tensor at dim 0, so apply squeeze() to get 3d tensor\n","        example[\"pixel_values\"] = torch.squeeze(self.transform(example[\"pixel_values\"]))\n","        # print(\"Afte transform\",example[\"pixel_values\"].shape)\n","\n","        if self.test_speed:\n","            print(\"Used time=\",time.time()-start_time)\n","            global used_times\n","            used_times.append(time.time()-start_time)\n","        return example\n","\n","\n","train_dataset = TextualInversionDataset(\n","      data_root=data_root,\n","      tokenizer=tokenizer,\n","      size=512,\n","      training_size=hyperparam[\"training_dataset_size\"],\n","      include_desc=hyperparam[\"include_summary\"]\n",")\n","def create_dataloader(train_batch_size=1):\n","    return torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True,pin_memory=True,num_workers=2)\n","print(\"Number of training examples used:\", len(train_dataset.df))\n","print(\"Number of templates:\",len(train_dataset.templates))\n","print(\"Training image size:\",train_dataset.size)\n","print(\"Train loader size:\", len(create_dataloader(hyperparam[\"train_batch_size\"])))\n","print()\n","print(\"Templates:\\n\",train_dataset.templates)"]},{"cell_type":"markdown","metadata":{"id":"GD5MU6EzFe27"},"source":["We have added the `placeholder_token` in the `tokenizer` so we resize the token embeddings here, this will a new embedding vector in the token embeddings for our `placeholder_token`"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T06:27:19.321146Z","iopub.status.busy":"2022-12-01T06:27:19.320455Z","iopub.status.idle":"2022-12-01T06:27:19.330092Z","shell.execute_reply":"2022-12-01T06:27:19.329133Z","shell.execute_reply.started":"2022-12-01T06:27:19.321107Z"},"id":"24-9I6mIoORT","outputId":"868320de-f46b-42e1-88b9-0a878ac78093","trusted":true},"outputs":[],"source":["text_encoder.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"markdown","metadata":{"id":"IA3Xj7gBFv-6"},"source":[" Initialise the newly added placeholder token with the embeddings of the initializer token"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T06:27:19.332058Z","iopub.status.busy":"2022-12-01T06:27:19.331564Z","iopub.status.idle":"2022-12-01T06:27:19.337659Z","shell.execute_reply":"2022-12-01T06:27:19.336623Z","shell.execute_reply.started":"2022-12-01T06:27:19.332021Z"},"id":"0mtxiZMNoQvE","trusted":true},"outputs":[],"source":["token_embeds = text_encoder.get_input_embeddings().weight.data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T06:27:19.339695Z","iopub.status.busy":"2022-12-01T06:27:19.339111Z","iopub.status.idle":"2022-12-01T06:27:21.417656Z","shell.execute_reply":"2022-12-01T06:27:21.416605Z","shell.execute_reply.started":"2022-12-01T06:27:19.339657Z"},"id":"0MTkwTNxpRnq","trusted":true},"outputs":[],"source":["#@title  Create noise scheduler\n","# noise_scheduler = DDPMScheduler(\n","#     beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, tensor_format=\"pt\"\n","# )\n","noise_scheduler=DDPMScheduler.from_config(pretrained_model_name_or_path, subfolder=\"scheduler\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"esfL5sitvy11","trusted":true},"outputs":[],"source":["#@title Visualize training result\n","#fix random seed by fixing latents\n","latents=None\n","def visualize_prompts(\n","    pipeline: StableDiffusionPipeline,\n","    summerize=False,\n","    include_desc=False,#include description\n","    max_length=15,#only when summerize=True\n","    legible_prompt=True,\n","    samples_per_prompt=3,\n","    img_size=512,\n","    inference_steps=75,\n","    save_to_drive=True,\n","    batch_generate=False\n","    ):\n","    if summerize==True:\n","      assert include_desc==True, \"include_desc is False, \\\n","      no summerization can be done without book description!\" \n","    if include_desc==True and batch_generate==True:\n","      #TODO: checkout the bug: passing tokenizer with padding=True to from_pretrained() does not solve this.\n","      print(\"Setting batch_generate to false since passing stacked descriptions of different length to model will cause error.\")\n","      print(\"---------------------------------------------\")\n","      batch_generate=False\n","    import gc\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    torch.cuda.memory_allocated()\n","    \n","    #fix random seed by fixing latents.\n","    #generate fixed latents if no latents exist\n","    global latents\n","    if latents==None or latents.shape[0]!=samples_per_prompt:\n","      generator = torch.Generator(device='cuda')\n","      generator = generator.manual_seed(global_seed)\n","      latents=torch.zeros(samples_per_prompt,\n","                           pipeline.unet.in_channels,img_size // 8, img_size // 8)\n","      for j in range(samples_per_prompt):\n","        latents[j,:,:,:] = torch.randn(\n","            (pipeline.unet.in_channels, img_size // 8, img_size // 8),\n","            generator = generator,\n","            device = 'cuda'\n","        )\n","    \n","\n","    import matplotlib.pyplot as plt,random\n","    #generate from test prompts only\n","    df=pd.read_csv(label_root+\"/df_test.csv\")\n","\n","    #set up figures\n","    dpi=plt.figure().dpi\n","    fig,axes=plt.subplots(len(test_templates),\n","                          samples_per_prompt,\n","                          figsize=(img_size/dpi*samples_per_prompt,\n","                                   img_size/dpi*len(test_templates))\n","                          )\n","    fig.subplots_adjust(wspace=0, hspace=0)#combind with axes[i][j].set_aspect('auto'); remove spacing\n","    # plt.suptitle(,y=0.89)\n","\n","    #fix random seed by fixing latents\n","    if include_desc:\n","      from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","      tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n","      if summerize:\n","          model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n","\n","\n","    for i in range(len(test_templates)):\n","      text=[]\n","      for j in range(samples_per_prompt):\n","        row=df.iloc[j]\n","        legible_text,author,title,description = (\"\",row['book_authors'], row['book_title'], row['book_desc'])\n","\n","        if legible_prompt:\n","            legible_text=\"legible text\"\n","        if summerize:\n","            inputs = tokenizer(description, max_length=1024, \n","                               return_tensors=\"pt\",truncation=True,padding=\"max_length\")\n","            summary_ids = model.generate(inputs['input_ids'], num_beams=3,\\\n","                                         min_length=2, max_length=max_length)\n","            description = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, \n","                                          clean_up_tokenization_spaces=False)[0]#batch_decode returns a list of strings; here len(list)=1, only one input string\n","        #get prompt\n","        template=test_templates[i]\n","        if include_desc:\n","          template+=summary_placeholders[i]#append new prompt to list\n","          template=template.format(legible_text,author,title,description)\n","          # print(\"before tokenizer:\", len(template))\n","          # #pad to the same length\n","          # inputs = tokenizer(template, max_length=1024, \n","          #                      return_tensors=\"pt\",truncation=True,padding=True)\n","          \n","          # template = tokenizer.batch_decode(inputs['input_ids'], skip_special_tokens=True, \n","          #                                 clean_up_tokenization_spaces=False)[0]\n","          # print(\"after:\",len(template))\n","          text += [template]\n","        else:\n","          text += [template.format(legible_text,author,title)]\n","\n","\n","      #inference \n","      from torch import autocast\n","      images=[]\n","      print(f\"Inference iteration {i}\")\n","\n","      with autocast(\"cuda\"):\n","        if batch_generate:#batch generation\n","          images=pipeline(text,height=img_size,width=img_size,\n","                          num_inference_steps=50, guidance_scale=7.5,\n","                          latents=latents).images\n","        else:#To avoid out of memory, generate one at a time\n","          for j in range(samples_per_prompt):\n","            images+=pipeline(text[j],height=img_size,\n","                              width=img_size,num_inference_steps=inference_steps, \n","                              guidance_scale=7.5,latents=latents[None,j]).images\n","                              \n","      try:\n","        axes[i][0].set_title(f\"Prompt {i}, legible={legible_prompt},summerize={summerize},include_desc={include_desc}\")\n","        for j in range(samples_per_prompt):\n","            axes[i][j].imshow(images[j])\n","            axes[i][j].set_aspect('auto')#remove spacing\n","      #single plot case\n","      except:\n","        axes[i].set_title(f\"Prompt {i}, legible={legible_prompt},summerize={summerize},include_desc={include_desc}\")\n","        #debug\n","        print(images[0])\n","        print(\"images:\",images)\n","        axes[i].imshow(images[0])\n","        axes[i].set_aspect('auto')\n","\n","    if save_to_drive:  \n","      #save fig with paramters\n","      img_name=f\"Generated_covers:legible={legible_prompt},summerize={summerize},\\\n","                include_desc={include_desc},max_length={max_length}.png\"\n","      path=\"./\"+img_name\n","      plt.savefig(path)\n","      fig.show()\n","    else:\n","      #save checkpoint generation results in wandb\n","      img_path=\"checkpoint_image_sample.jpg\"\n","      plt.savefig(img_path)\n","      from PIL import Image\n","      image=Image.open(img_path)\n","      wandb.log({\"examples\":wandb.Image(image)})"]},{"cell_type":"markdown","metadata":{"id":"YNuNDw0wNN5X"},"source":["### Define training function"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T06:30:09.829310Z","iopub.status.busy":"2022-12-01T06:30:09.828774Z","iopub.status.idle":"2022-12-01T06:30:09.877817Z","shell.execute_reply":"2022-12-01T06:30:09.876618Z","shell.execute_reply.started":"2022-12-01T06:30:09.829275Z"},"id":"djBS3343sIiY","outputId":"caded5c6-2951-466f-9524-9c958036b1d5","trusted":true},"outputs":[],"source":["%env WANDB_LOG_MODEL=true\n","def freeze_params(params):\n","    for param in params:\n","        param.requires_grad = False\n","\n","\n","\n","# Freeze all parameters except for the token embeddings in text encoder\n","# params_to_freeze = itertools.chain(\n","#     text_encoder.text_model.encoder.parameters(),\n","#     text_encoder.text_model.final_layer_norm.parameters(),\n","#     text_encoder.text_model.embeddings.position_embedding.parameters(),\n","# )\n","# freeze_params(params_to_freeze)\n","\n","\n","def training_function(\n","                    text_encoder, vae, unet,\n","                    resume=False,train_unet=False,train_text_encoder=True,\n","                    gradient_checkpointing=False,use_8bit_adam=True):\n","    logger = get_logger(__name__)#TODO: switch to wandb\n","    wandb.login(key='16d21dc747a6f33247f1e9c96895d4ffa5ea0b27',relogin=True)\n","    wandb.init(\n","           project=\"book_cover_generation\", \n","           config=hyperparam, \n","           name=\"stable_diffusion\",\n","           tags=[\"reverted to Kaggle version 10\",\"Simplified templates\", \"text_encoder_only\"],\n","           )\n","    \n","    #extract hyperparams\n","    train_batch_size = hyperparam[\"train_batch_size\"]\n","    gradient_accumulation_steps = hyperparam[\"gradient_accumulation_steps\"]\n","    learning_rate = hyperparam[\"learning_rate\"]\n","    num_train_epochs = hyperparam[\"epochs\"]\n","    output_dir = hyperparam[\"output_dir\"]\n","    weight_decay=hyperparam[\"weight_decay\"]\n","    accelerator = Accelerator(\n","        gradient_accumulation_steps=gradient_accumulation_steps,\n","    )\n","\n","    train_dataloader = create_dataloader(train_batch_size)\n","    if hyperparam[\"scale_lr\"]:\n","        learning_rate = (\n","            learning_rate * gradient_accumulation_steps * train_batch_size * accelerator.num_processes\n","        )\n","    print(\"lr after hyperparam[\\\"scale_lr\\\"]:\",learning_rate)\n","\n","    \n","\n","    #prepare models for training\n","    \n","    \n","    if train_text_encoder: \n","      text_encoder = accelerator.prepare(text_encoder)\n","      text_encoder.train()\n","      if gradient_checkpointing:\n","        text_encoder.gradient_checkpointing_enable()\n","    else:\n","      text_encoder.to(accelerator.device,dtype=torch.float16)\n","      freeze_params(text_encoder.parameters())\n","      text_encoder.eval()\n","\n","    if train_unet:\n","      unet=accelerator.prepare(unet)\n","      unet.train()\n","      if gradient_checkpointing:\n","        unet.enable_gradient_checkpointing()\n","      \n","    else:\n","      # Move models that don't need to be trained to device with fp16 \n","      unet.to(accelerator.device,dtype=torch.float16)\n","      freeze_params(unet.parameters())\n","      unet.eval()\n","\n","    freeze_params(vae.parameters())\n","    vae.to(accelerator.device,dtype=torch.float16)\n","    vae.eval()\n","\n","    # Initialize the optimizer\n","    print(f\"Train unet:{unet.training} || Train text_encoder:{text_encoder.training}\")\n","    param_list=[model.parameters() for model in [unet,text_encoder] if model.training]\n","    params_to_train=itertools.chain(*param_list)\n","     \n","    if use_8bit_adam:\n","      import bitsandbytes as bnb\n","      optimizer_class = bnb.optim.AdamW8bit\n","    else:\n","      optimizer_class = torch.optim.AdamW\n","\n","    optimizer = optimizer_class(\n","        params_to_train,\n","        lr=learning_rate,\n","        weight_decay=weight_decay\n","    )\n","    optimizer, train_dataloader = accelerator.prepare(optimizer, train_dataloader)\n","    scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=100,eta_min=1e-6,verbose=True)\n","    print(\"optimizer after wrapping using accelerator:\",optimizer)\n","\n","    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n","    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n","    # num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n","    max_train_steps = num_update_steps_per_epoch*num_train_epochs\n","\n","    print(\"num_update_steps_per_epoch:\",num_update_steps_per_epoch)\n","    print('num_train_epochs',num_train_epochs)\n","    print(\"accelerator.num_processes\",accelerator.num_processes)\n","\n","\n","    ###########\n","    # Train!  #\n","    ###########\n","    total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps\n","    print(\"Train!\")\n","    # logger.info(\"***** Running training *****\")\n","    # logger.info(f\"  Num examples = {len(train_dataset)}\")\n","    # logger.info(f\"  Instantaneous batch size per device = {train_batch_size}\")\n","    # logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n","    # logger.info(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n","    # logger.info(f\"  Total optimization steps = {max_train_steps}\")\n","    # Only show the progress bar once on each machine.\n","    progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n","    progress_bar.set_description(\"Max gradient update steps\")\n","    global_step = 0\n","    min_loss=1e9\n","    for epoch in range(num_train_epochs):\n","        text_encoder.train()\n","        epoch_loss=None\n","        for step, batch in enumerate(train_dataloader):\n","          from torch import autocast\n","          with autocast('cuda'):\n","            with accelerator.accumulate(text_encoder):\n","                # Convert images to latent space\n","                latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample().detach()\n","                latents = latents * 0.18215\n","\n","                # Sample noise that we'll add to the latents\n","                noise = torch.randn(latents.shape).to(latents.device)\n","                bsz = latents.shape[0]\n","                # Sample a random timestep for each image\n","                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device).long()\n","\n","                # Add noise to the latents according to the noise magnitude at each timestep\n","                # (this is the forward diffusion process)\n","                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n","\n","                # Get the text embedding for conditioning\n","                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n","\n","                # Predict the noise residual\n","                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n","                \n","                loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n","                #aggregate epoch training loss\n","                if not epoch_loss:\n","                  epoch_loss=loss.detach().item()\n","                else:\n","                  epoch_loss+=loss.detach().item()\n","                accelerator.backward(loss)\n","\n","\n","                #save best model every 1/2 epoch\n","                saves_per_epoch=2\n","                if (step+1)%int(len(train_dataloader)/saves_per_epoch)==0 or step+1==len(train_dataloader):\n","                  epoch_loss=epoch_loss/len(train_dataloader)*saves_per_epoch\n","                  wandb.log({\"mean_epoch_loss\":epoch_loss,\n","                            \"epoch\":int((step+1)/(len(train_dataloader)/saves_per_epoch))\n","                           })\n","                  if epoch_loss<min_loss:\n","                    min_loss=epoch_loss\n","                    epoch_loss=0\n","                    \n","                    print(f\"New min epoch loss {min_loss} at training step {step} of epoch {epoch}! Saving model...\")\n","                    if accelerator.is_main_process:\n","                      #without this, float16 weights will be saved, but unet and vae sub modules in Stablediffusion pipeline don't support that!\n","                      #you have to save float32 weights and then switch to float16 in from_pretrained()\n","                      unet.to(accelerator.device,dtype=torch.float32)\n","                      vae.to(accelerator.device,dtype=torch.float32)\n","                      pipeline = StableDiffusionPipeline(\n","                          text_encoder=accelerator.unwrap_model(text_encoder),\n","                          vae=vae,\n","                          unet=unet,\n","                          tokenizer=tokenizer,\n","                          scheduler=noise_scheduler,\n","                          safety_checker=StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n","                          feature_extractor=CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n","                      )\n","\n","\n","                      #save model\n","                      # pipeline.save_pretrained(output_dir)\n","                      #save model to wandb\n","                      pipeline.save_pretrained(output_dir)\n","                      del pipeline\n","                      artifact = wandb.Artifact(\"stable_diffusion_model\", \"model\")\n","                      artifact.add_dir(output_dir)\n","#                       pipeline = StableDiffusionPipeline.from_pretrained(\n","#                         \"./model\",\n","#                         torch_dtype=torch.float16,\n","#                       ).to('cuda')\n","                      #save generated image to wandb\n","                      #causes OOE \n","#                       visualize_prompts(\n","#                           pipeline,\n","#                           summerize=False,\n","#                           include_desc=False,\n","#                           legible_prompt=True,\n","#                           samples_per_prompt=1,\n","#                           save_to_drive=False,\n","#                           batch_generate=False,\n","#                           )\n","#                       del pipeline\n","\n","                      #switch back to float16 for training\n","                      unet.to(accelerator.device,dtype=torch.float16)\n","                      vae.to(accelerator.device,dtype=torch.float16)\n","                      \n","                      #save optimizer\n","                      try:\n","                        torch.save(optimizer.state_dict(),os.path.join(output_dir,\"optimizer_state_dict.pt\"))\n","                      except:\n","                        optimizer.save_state(output_dir)\n","\n","\n","                optimizer.step()\n","                optimizer.zero_grad()\n","            \n","            # Checks if the accelerator has performed an optimization step behind the scenes\n","            if accelerator.sync_gradients:\n","                progress_bar.update(1)\n","                global_step += 1\n","            scheduler.step()\n","            logs = {\"loss\": loss.detach().item(),\"epoch\": epoch,\"step\": f\"{step}/{len(train_dataloader)}\"}\n","            wandb.log(logs)\n","            progress_bar.set_postfix(**logs)\n","\n","            if global_step >= max_train_steps:\n","                break\n","          #for distributed training \n","          accelerator.wait_for_everyone()\n","    wandb.run.log_artifact(artifact)\n","\n","\n","      \n","        \n","          # Also save the newly trained embeddings\n","          # learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_id]\n","          # learned_embeds_dict = {placeholder_token: learned_embeds.detach().cpu()}\n","          # torch.save(learned_embeds_dict, os.path.join(output_dir, \"learned_embeds.bin\"))"]},{"cell_type":"markdown","metadata":{"id":"pem_EG-C6OE9"},"source":["# Train model!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T06:30:16.897179Z","iopub.status.busy":"2022-12-01T06:30:16.896798Z","iopub.status.idle":"2022-12-01T06:38:48.543952Z","shell.execute_reply":"2022-12-01T06:38:48.537206Z","shell.execute_reply.started":"2022-12-01T06:30:16.897146Z"},"id":"jXi0NdsyBA4S","outputId":"ac99079d-e0d2-45ea-ae00-050ac2b1ffb2","trusted":true},"outputs":[],"source":["\n","import gc\n","gc.collect()\n","torch.cuda.empty_cache()\n","torch.cuda.memory_allocated()\n","\n","\n","import accelerate\n","#args in the second line:\n","#resume,train_unet,train_text_encoder,gradient_checkpointing,use_8bit_adam\n","accelerate.notebook_launcher(training_function, args=(text_encoder, vae, unet, \n","                            False,hyperparam[\"train_unet\"],hyperparam[\"train_text_encoder\"],False,True))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-12-01T06:38:48.557517Z","iopub.status.idle":"2022-12-01T06:38:48.558013Z","shell.execute_reply":"2022-12-01T06:38:48.557786Z","shell.execute_reply.started":"2022-12-01T06:38:48.557759Z"},"trusted":true},"outputs":[],"source":["# #shutdown notebook \n","# from IPython.display import Javascript\n","# print(“Shutdown”)\n","# Javascript(\"Ipython.notebook.session.delete()\")"]},{"cell_type":"markdown","metadata":{},"source":["### Load from  checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2CMlPbOeEC09","trusted":true},"outputs":[],"source":[" #@title Fine tune result evaluation\n","output_dir=hyperparam[\"output_dir\"]\n","if os.path.isdir(output_dir):\n","    #load from local checkpoint\n","    try:\n","      pipeline = StableDiffusionPipeline.from_pretrained(\n","                hyperparam[\"output_dir\"],\n","                torch_dtype=torch.float16,\n","                safety_checker = None\n","              ).to('cuda')\n","      print(f\"Built pipeline from {output_dir}\")\n","    except:\n","      #manual pipeline\n","      accelerator = Accelerator()\n","\n","      if not \"text_encoder\" in globals():\n","        text_encoder = CLIPTextModel.from_pretrained(\n","            hyperparam[\"output_dir\"], subfolder=\"text_encoder\"\n","            , use_auth_token=True\n","        )\n","      if not \"vae\" in globals():\n","        vae = AutoencoderKL.from_pretrained(\n","            hyperparam[\"output_dir\"], subfolder=\"vae\"\n","            , use_auth_token=True\n","        )\n","        vae.to(accelerator.device,dtype=torch.float32)\n","      if not \"unet\" in globals():\n","        unet = UNet2DConditionModel.from_pretrained(\n","            hyperparam[\"output_dir\"], subfolder=\"unet\"\n","            , use_auth_token=True\n","        )\n","        unet.to(accelerator.device,dtype=torch.float32)\n","      if not \"tokenizer\" in globals():\n","        tokenizer = CLIPTokenizer.from_pretrained(\n","        hyperparam[\"output_dir\"],\n","        subfolder=\"tokenizer\",\n","        use_auth_token=True,\n","      )\n","\n","      pipeline = StableDiffusionPipeline(\n","                      text_encoder=text_encoder,\n","                      vae=vae,\n","                      unet=unet,\n","                      tokenizer=tokenizer,\n","                      scheduler=PNDMScheduler(\n","                          beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True\n","                      ),\n","                      safety_checker=None,\n","                      feature_extractor=CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n","\n","      )\n","      print(f\"Built pipeline from components from {output_dir}\")\n","else:\n","    #load from wandb checkpoint\n","    os.environ[\"WANDB_API_KEY\"]=\"16d21dc747a6f33247f1e9c96895d4ffa5ea0b27\"\n","    #can't use artifact in offline mode\n","#     os.environ['WANDB_MODE'] = 'online'\n","\n","    with wandb.init(project=\"book_cover_generation\") as run:\n","      my_model_artifact =run.use_artifact(\"stable_diffusion_model:v17\")\n","      # Download model weights to a folder and return the path\n","      model_dir = my_model_artifact.download()\n","\n","      # Load your Hugging Face model from that folder\n","      #  using the same model class\n","      tokenizer = CLIPTokenizer.from_pretrained(\n","        model_dir,\n","        subfolder=\"tokenizer\",\n","        use_auth_token=True,\n","        Padding=\"max_length\",\n","        Truncation=True,\n","      )\n","      pipeline = StableDiffusionPipeline.from_pretrained(\n","          model_dir,\n","          torch_dtype=torch.float16,\n","          safety_checker=None,\n","          tokenizer=tokenizer#enable padding\n","          ).to('cuda')\n","    print('Load model from wandb cloud checkpoint')\n"]},{"cell_type":"markdown","metadata":{"id":"yY7vzTj4HIiK"},"source":["## Visualize different prompt strategies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NWJLAxXRbvPj","outputId":"83fb187d-4316-43c6-f4c2-d93d7d029c75","trusted":true},"outputs":[],"source":["visualize_prompts(pipeline,summerize=False,include_desc=False,legible_prompt=False)"]},{"cell_type":"markdown","metadata":{"id":"nCJdBiUWLD4Q"},"source":["### Test effectiveness of summerization with other factors controlled for."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GLfsx-3HDMx2","outputId":"ad3d29da-256c-4329-d91b-5572f80f2386","trusted":true},"outputs":[],"source":["visualize_prompts(pipeline,summerize=True,include_desc=True,legible_prompt=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QeoyImZMtFKH","outputId":"4f63367e-6b0f-4d2b-8134-b1dd0df6a929","trusted":true},"outputs":[],"source":["visualize_prompts(pipeline,summerize=False,include_desc=True,legible_prompt=False)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"30H2mXBsKy4r"},"source":["## Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BXW02rvFKt3e","trusted":true},"outputs":[],"source":["# /kaggle/input/goodreads-best-book-cleaned-version/df_test.csv\n","# /kaggle/input/goodreads-best-books/df_test.csv\n","# #@title CLIP score\n","# import gradio as gr\n","# from transformers import CLIPProcessor, CLIPModel\n","\n","# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n","# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n","\n","\n","# def calculate_score(image, text):\n","#     labels = text.split(\";\")\n","#     labels = [l.strip() for l in labels]\n","#     labels = list(filter(None, labels))\n","#     if len(labels) == 0:\n","#         return dict()\n","#     inputs = processor(text=labels, images=image, return_tensors=\"pt\", padding=True)\n","#     outputs = model(**inputs)\n","#     logits_per_image = outputs.logits_per_image.detach().numpy()\n","\n","#     results_dict = {\n","#         label: score / 100.0 for label, score in zip(labels, logits_per_image[0])\n","#     }\n","#     return results_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vw2LVBFvLXrq","trusted":true},"outputs":[],"source":["#@title FID score\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E3UREGd7EkLh","outputId":"759772e3-978b-4f23-96e7-93bab5c71526","trusted":true},"outputs":[],"source":["#@title Run the Stable Diffusion pipeline\n","#@markdown Don't forget to use the placeholder token in your prompt\n","import gc\n","gc.collect()\n","torch.cuda.empty_cache()\n","torch.cuda.memory_allocated()\n","\n","from torch import autocast\n","# prompt = \"a grafitti in a wall with a <cat-toy> on it\" #@param {type:\"string\"}\n","prompt=\"Clear, highly detailed book cover with title とある魔術の禁書目録 2\"\n","# prompt=\"Clear, highly detailed book cover with description \"+book_df.loc[7202]['book_desc']\n","\n","num_samples = 2 #@param {type:\"number\"}\n","num_rows = 2 #@param {type:\"number\"}\n","width=512\n","height=512\n","all_images = [] \n","\n","for _ in range(num_rows):\n","    with autocast(\"cuda\"):\n","        images = pipeline([prompt] * num_samples,height=height,width=width,num_inference_steps=50, guidance_scale=7.5).images\n","        all_images.extend(images)\n","\n","grid = image_grid(all_images, num_samples, num_rows)\n","grid"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13 (default, Oct  4 2022, 14:00:32) \n[GCC 9.4.0]"},"vscode":{"interpreter":{"hash":"9ac03a0a6051494cc606d484d27d20fce22fb7b4d169f583271e11d5ba46a56e"}}},"nbformat":4,"nbformat_minor":4}
