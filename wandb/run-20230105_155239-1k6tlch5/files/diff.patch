diff --git a/__pycache__/stable-diffusion-for-book-cover-generation.cpython-38.pyc b/__pycache__/stable-diffusion-for-book-cover-generation.cpython-38.pyc
new file mode 100644
index 0000000..3d5b6c3
Binary files /dev/null and b/__pycache__/stable-diffusion-for-book-cover-generation.cpython-38.pyc differ
diff --git a/__pycache__/training.cpython-38.pyc b/__pycache__/training.cpython-38.pyc
index 2544b14..516836c 100644
Binary files a/__pycache__/training.cpython-38.pyc and b/__pycache__/training.cpython-38.pyc differ
diff --git a/__pycache__/training.cpython-39.pyc b/__pycache__/training.cpython-39.pyc
index 2c57668..6998782 100644
Binary files a/__pycache__/training.cpython-39.pyc and b/__pycache__/training.cpython-39.pyc differ
diff --git a/training.py b/training.py
index b569e3c..91208bc 100644
--- a/training.py
+++ b/training.py
@@ -25,15 +25,17 @@ import wandb
 import subprocess
 from contextlib import contextmanager,nullcontext
 parser = argparse.ArgumentParser()
-parser.add_argument("--lr",help="learning rate",default=5e-6,type=int)
+parser.add_argument("--lr",help="learning rate",default=5e-6,type=float)
 parser.add_argument("--epochs",default=12,type=int)
 parser.add_argument("--train_unet",help="whether to train Unet or not",default=False,type=bool)
 parser.add_argument("--decay",help="weight_decay",default=1e-2,type=int)
 parser.add_argument("--train_text_encoder",default=True,type=bool)
 parser.add_argument("--data_root",default="../book dataset",type=str)
-parser.add_argument("--num_examples",default=8000,type=int,help="number of training examples")
-parser.add_argument("--num_devices",default=3,type=int)
-parser.add_argument("--gradient_acc_steps",default=8,type=int)
+parser.add_argument("--num_examples",default=10000,type=int,help="number of training examples")
+parser.add_argument("--num_gpus",default=3,type=int)
+parser.add_argument("--resume_id",default=None,type=int,help="wandb run id of the model to be resumed")
+parser.add_argument("--wandb_key",default=None,help="wandb id for syncing training. If not provided, the checkpoint will not be saved in cloud")
+parser.add_argument("--gradient_acc_steps",default=16,type=int)
 args = parser.parse_args()
 def image_grid(imgs, rows, cols):
     assert len(imgs) == rows*cols
@@ -77,8 +79,8 @@ book_cover_templates=[#the first entry is for "highly legible text"
 #     "A {} vivid book cover with author {}, book title {} ",
     "A  {} book cover with author name:{}, book title: {}",
 # #     "We are going to create a clear, {}, highly detailed book cover with author named {}, and book title is '{}'",
-#     "An intricate {}, book cover including book author:{}, book title: '{}'",
-#     "A detailed, {}, book cover with {} ,written by author {}",
+     "An intricate {} book cover including book author:{}, book title: '{}'",
+#     "A detailed, {} book cover with {} ,written by author {}",
 #     "A creative, colorful {}, book cover written by {}. The book title is {}, ",
 #     "A {} old-fashioned, plain book cover written by {}. The book title is {}",
 #     "A simple, {}, old-fashioned book cover with author name {}, book title {} ",
@@ -185,7 +187,7 @@ hyperparam = {
     "weight_decay": args.decay,
     # "noise_scheduler": "DDIM",
     "pretrained_model_name_or_path": pretrained_model_name_or_path,
-    "output_dir": "./model",
+    "output_dir": "./model_ckpt",
     "training_dataset_size":args.num_examples,
     "train_unet": args.train_unet,
     "train_text_encoder": args.train_text_encoder,
@@ -440,7 +442,8 @@ def visualize_prompts(
       plt.savefig(img_path)
       from PIL import Image
       image=Image.open(img_path)
-      wandb.log({"examples":wandb.Image(image)})
+      if args.wandb_key:
+        wandb.log({"examples":wandb.Image(image)})
 
 
 def training_function(
@@ -460,7 +463,8 @@ def training_function(
     from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
     from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer,TrainingArguments   
     from torch import autocast
-     # Load models and create wrapper for stable diffusion
+
+    # Load models and create wrapper for stable diffusion
     text_encoder = CLIPTextModel.from_pretrained(
         pretrained_model_name_or_path, subfolder="text_encoder"
         , use_auth_token=True
@@ -486,16 +490,15 @@ def training_function(
     torch.cuda.memory_allocated() 
     #set random seed     
     set_seed(global_seed)
-
-    # logger = get_logger(__name__)
-    wandb.login(key='16d21dc747a6f33247f1e9c96895d4ffa5ea0b27',relogin=True)
-    wandb.init(
-           project="book_cover_generation", 
-           config=hyperparam, 
-           name="stable_diffusion",
-           tags=["reverted to Kaggle version 10","Simplified templates", "text_encoder_only"],
-           )
-    
+    if args.wandb_key:
+        wandb.login(key=args.wandb_key,relogin=True)
+        wandb.init(
+               project="book_cover_generation", 
+               config=hyperparam, 
+               name="stable_diffusion",
+               tags=["reverted to Kaggle version 10","Simplified templates", "text_encoder_only"],
+               )
+        
     #get hyperparams
     train_batch_size = hyperparam["train_batch_size"]
     gradient_accumulation_steps = hyperparam["gradient_accumulation_steps"]
@@ -549,7 +552,6 @@ def training_function(
     print(f"Train unet:{unet.training} || Train text_encoder:{text_encoder.training}")
     param_list = [model.parameters() for model in [unet,text_encoder] if model.training]
     params_to_train = itertools.chain(*param_list)
-     
     if use_8bit_adam:
       import bitsandbytes as bnb
       optimizer_class = bnb.optim.AdamW8bit
@@ -562,7 +564,7 @@ def training_function(
         weight_decay=weight_decay
     )
     
-    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=100,eta_min=1e-6,verbose=True)
+    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=100,eta_min=1e-6,verbose=False)
     optimizer, train_dataloader, scheduler = accelerator.prepare(optimizer, train_dataloader,scheduler)
     print("optimizer after wrapping using accelerator:",optimizer)
 
@@ -574,25 +576,19 @@ def training_function(
     print("num_update_steps_per_epoch:",num_update_steps_per_epoch)
     print('num_train_epochs',num_train_epochs)
     print("accelerator.num_processes",accelerator.num_processes)
-
+    print("Number of training examples:",args.num_examples)
     ###########
     # Train!  #
     ###########
     total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps
     print("Train!")
-    # logger.info("***** Running training *****")
-    # logger.info(f"  Num examples = {len(train_dataset)}")
-    # logger.info(f"  Instantaneous batch size per device = {train_batch_size}")
-    # logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
-    # logger.info(f"  Gradient Accumulation steps = {gradient_accumulation_steps}")
-    # logger.info(f"  Total optimization steps = {max_train_steps}")
     # Only show the progress bar once on each machine.
     progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)
     progress_bar.set_description("Max gradient update steps")
     global_step = 0
     min_loss=1e9
     for epoch in range(num_train_epochs):
-        epoch_loss=None
+        mean_loss=None
         for step, batch in enumerate(train_dataloader):
           with autocast('cuda'):
             context1 = accelerator.accumulate(unet) if args.train_unet else nullcontext()
@@ -620,26 +616,27 @@ def training_function(
                 
                 loss = F.mse_loss(noise_pred, noise, reduction="none").mean([1, 2, 3]).mean()
                 #aggregate epoch training loss
-                if not epoch_loss:
-                  epoch_loss = loss.detach().item()
+                if not mean_loss:
+                  mean_loss = loss.detach().item()
                 else:
-                  epoch_loss += loss.detach().item()
+                  mean_loss += loss.detach().item()
                 accelerator.backward(loss)
 
-                #save best model every 1/2 epoch
-                saves_per_epoch=2
+                #save best model every 1/4 epoch
+                saves_per_epoch = 4
                 if (step+1)%int(len(train_dataloader)/saves_per_epoch)==0 or step+1==len(train_dataloader):
-                  epoch_loss=epoch_loss/len(train_dataloader)*saves_per_epoch
-                  wandb.log({"mean_epoch_loss":epoch_loss,
-                            "epoch":int((step+1)/(len(train_dataloader)/saves_per_epoch))
-                           })
-                  if epoch_loss<min_loss:
-                    min_loss=epoch_loss
-                    epoch_loss=0
+                  mean_loss = mean_loss/len(train_dataloader)*saves_per_epoch
+                  if args.wandb_key:
+                    wandb.log({"mean_epoch_loss":mean_loss,
+                                "epoch":int((step+1)/(len(train_dataloader)/saves_per_epoch))
+                               })
+                  if mean_loss<min_loss:
+                    min_loss = mean_loss
+                    mean_loss = 0
                     
-                    print(f"New min epoch loss {min_loss} at training step {step} of epoch {epoch}! Saving model...")
+                    print(f"New min loss {min_loss} at training step {step} of epoch {epoch}! Saving model...")
                     if accelerator.is_main_process:
-                      #without this, float16 weights will be saved, but unet and vae sub modules in Stablediffusion pipeline don't support that!
+                      #without this, fp16 weights will be saved, but the Unet and VAE sub modules in StableDiffusion pipeline don't support fp16!
                       #you have to save float32 weights and then switch to float16 in from_pretrained()
                       unet.to(accelerator.device,dtype=torch.float32)
                       vae.to(accelerator.device,dtype=torch.float32)
@@ -658,8 +655,9 @@ def training_function(
                       #save model to wandb
                       pipeline.save_pretrained(output_dir)
                       del pipeline
-                      artifact = wandb.Artifact("stable_diffusion_model", "model")
-                      artifact.add_dir(output_dir)
+                      if args.wandb_key:
+                        artifact = wandb.Artifact("stable_diffusion_model", "model")
+                        artifact.add_dir(output_dir)
 #                       pipeline = StableDiffusionPipeline.from_pretrained(
 #                         "./model",
 #                         torch_dtype=torch.float16,
@@ -696,21 +694,23 @@ def training_function(
                 global_step += 1
             scheduler.step()
             logs = {"loss": loss.detach().item(),"epoch": epoch,"step": f"{step}/{len(train_dataloader)}"}
-            wandb.log(logs)
+            if args.wandb_key:    
+                wandb.log(logs)
             progress_bar.set_postfix(**logs)
 
             if global_step >= max_train_steps:
                 break
           #for distributed training 
           accelerator.wait_for_everyone()
-    wandb.run.log_artifact(artifact)
-    subprocess.run(["rm","-r",output_dir])
-        
+        if args.wandb_key:
+            wandb.run.log_artifact(artifact)
+            subprocess.run(["rm","-r",output_dir])
+            
 ### Train model!
 from accelerate import notebook_launcher 
 notebook_launcher(training_function, args=( 
                                         False,hyperparam["train_unet"],hyperparam["train_text_encoder"],False,True),
-                                        num_processes=args.num_devices,mixed_precision="fp16"
+                                        num_processes=args.num_gpus,mixed_precision="fp16"
                                         )
 
 ### Load from  checkpoint
diff --git a/wandb/debug-cli.wenxuan.log b/wandb/debug-cli.wenxuan.log
new file mode 100644
index 0000000..e69de29
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
new file mode 120000
index 0000000..012d23e
--- /dev/null
+++ b/wandb/debug-internal.log
@@ -0,0 +1 @@
+run-20230105_155239-1k6tlch5/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
new file mode 120000
index 0000000..deae3ad
--- /dev/null
+++ b/wandb/debug.log
@@ -0,0 +1 @@
+run-20230105_155239-1k6tlch5/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
new file mode 120000
index 0000000..827f14c
--- /dev/null
+++ b/wandb/latest-run
@@ -0,0 +1 @@
+run-20230105_155239-1k6tlch5
\ No newline at end of file
